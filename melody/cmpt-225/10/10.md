---
layout: simple
math: true
---

Transciber's notes:
* $$\iff$$ is "if and only if"
* $$\implies$$ is "implies"
* $$\forall$$ is "for all"
* $$\exists$$ is "exists"

# Big-Oh--Part I

CMPT 225

## Slide 1

* Recall: if `f,g` are functions `f:N->N`,`g:N->N`
* f is `O(g)` means there are constants $$n_{0},c > 0$$
s.t. for every $$n>n_{0},f(n) \leq \text{c.g}(n)$$
* that is for all but finitely many "small" values of n: $$f(n) \leq cg(n)$$
* or f grows no faster than g (asymtotically)
* we typically write $$f(n) = O(g(n))$$

## Consider `f(n)`,`g(n)`:

Claim: $$f(n) = O(g(n))$$

Why: 

* When $$f(n) > g(n)$$ for all n?
* But $$f(n) < 2g(n)$$ for almost all values of `n`.
* choose $$n_{0}$$ to "exclude" the real values of `n`.
* Now: $$f(n) < 2g(n)$$ for all $$n>n_{0}$$
* So: $$f(n) = O(g(n))$$

## Also:

(A graph a cannot transcribe because the notation doesn't make any sense. All future graphs have complexity on the y axis and the value of n [input to the function] as the x asxis.)

Claim: f(n) is **not** O(g(n))

However large we choose $$c,n_{0}$$, there will be some k (larger than $$n_0$$) s.d.: $$n > k \implies f(n) > c g(n)$$

## Consider `O(1)` (i.e. $$g(n)=1$$)

$$f(n) = O(1)$$ if $$\exists n_{0},c > 0$$ s.t. $$\forall n>n_{0} \space \space f(n) \leq c\times 1$$

(Another graph that makes no Goddamn sense whatsoever.)

* for every $$n>n_{0}, f(n)<c$$ <-- >-->
* so, f(n) grwos no faster than a constant
* so f(n) is asymptotically bounded by a constant

## The constant does not matter


Fact: $$f(n) = O(1) \iff f(n)=O(\text{10}^{27}) \iff f(n) = O(\frac{1}{\text{10}^{27}})$$

Suppose: $$f(n) = O(\text{10}^{27})$$ \*

Claim: $$f(n)$$ is also $$O(\frac{1}{\text{10}^{27}})$$

\* means $$\exists n_{0},c > 0$$ s.t. $$n > n_{0} \implies f(n) \leq c\times\text{10}^{27}$$

Want to show: $$\exists n_{0},c^{1}>0$$ s.t. $$n>n_{0} \implies f(n) \leq c^{1}\times \frac{1}{\text{10}^{27}}$$

Choose $$c^1$$ big enough that $$c^{1} \frac{1}{\text{10}^{27}} \leq c \times \text{10}^{27}$$

e.g: $$c^{1} = c \times \text{10}^{54}$$

Then: 

{% katex display %}
\forall n>n_{0}, f(n) \leq c \times \text{10}^{27} \\
\leq c \times \text{10}^{-27} - \text{10}^{54}\\
\leq c^1 - \text{10}^{-27}
{% endkatex %}

(54-27=27)

So: $$f(n)=O(\text{10}^{-27})$$

## Asymtotic Notation (e.g. Big-Oh)

* Is *not* "about" algorithm
* *Is* a tool for describing (growth of) functions 
* It is useful for describing functions related to algorithms + data structures, e.g.:
  * minimum or maximum time taken
  * minimum or maximum space needed
  * etc.
* We use it so often for worst-case time for an algorithm that we often leave implciit a statement like "let T(n) be the max time taken by algorithm A as an input of size as most n." This statement is essential.

## Ex. *Complexity of Palindrome Checking*

* using a stack & queue
* Algorithm:
  1. Insert all tokens into a stack & a queue
  2. Repeat pop one token; dequeue one token. if different report 'no'.
* size of input = number of symbols or token
* each token is (all together O(1)):
  * pushed on the stack, O(1)
  * enqueued on the queue, O(1)
  * popped off the stack, O(1)
  * dequeued from the queue, O(1)
  * compared to one other token, O(1)
* $$\text{n tokens} \implies n \times O(1)$$ time in total
* So: $$T(n) = n \times O(1) = O(n)$$

## What does $$n\times O(1) = O(n)$$ mean?

It means: $$f(n) = O(1) \iff n\times f(n) = O(n)$$

To see if it is true: 

{% katex display %}
f(n) = O(1) \iff \exists c>0 \space \text{s.t.} \space f(n) < c, \text{for any} \space n \in \Z\\
\iff \exists c>0 \text{ s.t. } n\times f(n) < c\times n, \text{for any } n \in \N\\
\iff n\times f(n) = O(n)
{% endkatex %}

## End
